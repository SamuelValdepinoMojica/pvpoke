{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries and Environment\n",
    "Import Stable Baselines3, the custom PVPokeEnv, and other required libraries. Configure environment variables and connect to the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the server.\n",
      "WebSocket connection closed.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import websockets\n",
    "from pydantic import BaseModel\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import json\n",
    "from ClassPVPOKE import PVPokeEnv\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "# Create an instance of the environment and connect to the server\n",
    "env = PVPokeEnv(\"ws://localhost:8000/ws\", \"notebook\", \"pvpoke\", battle_format=\"1v1\")\n",
    "env.loop.run_until_complete(env.connect())\n",
    "\n",
    "# Check if the environment follows the Gym API\n",
    "check_env(env)\n",
    "\n",
    "# Close the environment connection\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Configure the Agent PPO\n",
    "Create and configure a Stable Baselines3 agent (like PPO or DQN) with appropriate hyperparameters for the PVPoke environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting experiment: DQN/Mantine-Gligar_256x6_lr0.0001_100k\n",
      "cuda\n",
      "Connected to the server.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a139f4075f490ab5d4e429fcd40c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished training DQN/Mantine-Gligar_256x6_lr0.0001_100k. Model saved.\n",
      "Closing environments for 256x6_lr0.0001...\n",
      "WebSocket connection closed.\n",
      "\n",
      "Starting experiment: DQN/Mantine-Gligar_512x6_lr0.0001_100k\n",
      "cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b24523623beb4045bec40ef70a5fa810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the server.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished training DQN/Mantine-Gligar_512x6_lr0.0001_100k. Model saved.\n",
      "Closing environments for 512x6_lr0.0001...\n",
      "WebSocket connection closed.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "from ClassPVPOKE import PVPokeEnv\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import torch\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Configuración de evaluación\n",
    "EVAL_EVERY = 20000\n",
    "TOTAL_TIMESTEPS = 2000000\n",
    "\n",
    "# Parámetros para grid\n",
    "architectures = [\n",
    "    #([128, 128], \"128x2\"),\n",
    "    #([256, 256], \"256x2\"),\n",
    "    #([512, 512], \"512x2\"),\n",
    "    #([128]*4, \"128x4\"),\n",
    "    #([256]*4, \"256x4\"),\n",
    "    #([512]*4, \"512x4\"),\n",
    "    #([128]*6, \"128x6\"),\n",
    "    ([256]*6, \"256x6\"),\n",
    "    ([512]*6, \"512x6\"),\n",
    "]\n",
    "learning_rates = [ 1e-4]\n",
    "\n",
    "# Info general\n",
    "pokemon_matchup = \"Mantine-Gligar\"\n",
    "battle_format = \"1v1\"\n",
    "model_type = \"DQN\"\n",
    "buffer = \"100k\"\n",
    "train_frec = \"4\"\n",
    "step = \"2m\"\n",
    "\n",
    "for arch, arch_name in architectures:\n",
    "    for lr in learning_rates:\n",
    "        # Nombre único de experimento\n",
    "        exp_id = f\"{arch_name}_lr{lr}\"\n",
    "        experiment_name = f\"{model_type}/{pokemon_matchup}_{exp_id}_{buffer}\"\n",
    "        log_dir = f\"./logs/{battle_format}/{train_frec}/{step}/{experiment_name}\"\n",
    "        models_dir = f\"./models/{battle_format}/{train_frec}/{step}/{model_type.lower()}/{pokemon_matchup}/{exp_id}_{buffer}\"\n",
    "\n",
    "        print(f\"\\nStarting experiment: {experiment_name}\")\n",
    "        print(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            # Conexión al entorno\n",
    "            env = PVPokeEnv(\"ws://localhost:8000/ws\", \"notebook\", \"pvpoke\", battle_format=\"1v1\")\n",
    "            env.loop.run_until_complete(env.connect())\n",
    "            env = Monitor(env)\n",
    "\n",
    "            # Definición del modelo\n",
    "            model = DQN(\n",
    "                policy=\"MlpPolicy\",\n",
    "                env=env,\n",
    "                learning_rate=lr,\n",
    "                buffer_size=100000,\n",
    "                learning_starts=10000,\n",
    "                batch_size=128,\n",
    "                gamma=0.999,\n",
    "                train_freq=4,\n",
    "                gradient_steps=1,\n",
    "                target_update_interval=1000,\n",
    "                exploration_fraction=0.2,\n",
    "                exploration_initial_eps=1.0,\n",
    "                exploration_final_eps=0.05,\n",
    "                tensorboard_log=log_dir,\n",
    "                policy_kwargs={\"net_arch\": arch},\n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                \n",
    "            )\n",
    "\n",
    "            checkpoint_callback = CheckpointCallback(\n",
    "                save_freq=EVAL_EVERY,\n",
    "                save_path=models_dir,\n",
    "                name_prefix='dqn_pvpoke'\n",
    "            )\n",
    "\n",
    "            eval_callback = EvalCallback(\n",
    "                env,\n",
    "                best_model_save_path=models_dir,\n",
    "                log_path=log_dir,\n",
    "                eval_freq=EVAL_EVERY,\n",
    "                deterministic=True,\n",
    "                render=False,\n",
    "                verbose=0,\n",
    "                n_eval_episodes=100,\n",
    "            )\n",
    "\n",
    "            # Entrenamiento\n",
    "            model.learn(\n",
    "                total_timesteps=TOTAL_TIMESTEPS,\n",
    "                callback=[checkpoint_callback, eval_callback],\n",
    "                progress_bar=True\n",
    "            )\n",
    "\n",
    "            # Guardado final\n",
    "            model.save(f\"{models_dir}/dqn_pvpoke_final\")\n",
    "            print(f\"✅ Finished training {experiment_name}. Model saved.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in {experiment_name}: {e}\")\n",
    "\n",
    "        finally:\n",
    "            print(f\"Closing environments for {exp_id}...\")\n",
    "            env.close()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizacion con Optuna "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-28 10:37:14,752] A new study created in memory with name: dqn_pvpoke_optuna\n",
      "c:\\Users\\samue\\miniconda3\\envs\\gym\\Lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [128, 128] which is of type list.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\samue\\miniconda3\\envs\\gym\\Lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [256, 256] which is of type list.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\samue\\miniconda3\\envs\\gym\\Lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [128, 128, 128] which is of type list.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\samue\\miniconda3\\envs\\gym\\Lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [256, 256, 256] which is of type list.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando optimización con 15 pruebas\n",
      "\n",
      "=== Iniciando Prueba 0 ===\n",
      "Parámetros: {'learning_rate': 0.0002769652880017355, 'buffer_size': 50000, 'learning_starts': 1361, 'batch_size': 32, 'gamma': 0.9863536870153633, 'train_freq': 4, 'target_update_interval': 200, 'exploration_fraction': 0.24416303764104672, 'exploration_final_eps': 0.0352729208634948, 'net_arch': [256, 256]}\n",
      "Connected to the server.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e98c43b7422047edbcedd0096f7fa8e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-28 10:49:31,389] Trial 0 finished with value: -1.7576924999999999 and parameters: {'learning_rate': 0.0002769652880017355, 'buffer_size': 50000, 'learning_starts': 1361, 'batch_size': 32, 'gamma': 0.9863536870153633, 'train_freq': 4, 'target_update_interval': 200, 'exploration_fraction': 0.24416303764104672, 'exploration_final_eps': 0.0352729208634948, 'net_arch': [256, 256]}. Best is trial 0 with value: -1.7576924999999999.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prueba 0 - Recompensa media: -1.76 ± 0.12\n",
      "WebSocket connection closed.\n",
      "Entorno cerrado para prueba 0\n",
      "\n",
      "=== Iniciando Prueba 1 ===\n",
      "Parámetros: {'learning_rate': 0.00020329643334753301, 'buffer_size': 20000, 'learning_starts': 1613, 'batch_size': 32, 'gamma': 0.9839722894117535, 'train_freq': 4, 'target_update_interval': 800, 'exploration_fraction': 0.23518708110981204, 'exploration_final_eps': 0.07239082022843095, 'net_arch': [128, 128]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samue\\miniconda3\\envs\\gym\\Lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [128, 128] which is of type list.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\samue\\miniconda3\\envs\\gym\\Lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [256, 256] which is of type list.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\samue\\miniconda3\\envs\\gym\\Lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [128, 128, 128] which is of type list.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\samue\\miniconda3\\envs\\gym\\Lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [256, 256, 256] which is of type list.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "581c406f987f4e8e8a501cf1860cd530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-28 11:00:04,635] Trial 1 finished with value: -1.8823073000000001 and parameters: {'learning_rate': 0.00020329643334753301, 'buffer_size': 20000, 'learning_starts': 1613, 'batch_size': 32, 'gamma': 0.9839722894117535, 'train_freq': 4, 'target_update_interval': 800, 'exploration_fraction': 0.23518708110981204, 'exploration_final_eps': 0.07239082022843095, 'net_arch': [128, 128]}. Best is trial 0 with value: -1.7576924999999999.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prueba 1 - Recompensa media: -1.88 ± 0.01\n",
      "WebSocket connection closed.\n",
      "Entorno cerrado para prueba 1\n",
      "\n",
      "=== Iniciando Prueba 2 ===\n",
      "Parámetros: {'learning_rate': 0.0002126835385636654, 'buffer_size': 20000, 'learning_starts': 1572, 'batch_size': 32, 'gamma': 0.983206386839582, 'train_freq': 4, 'target_update_interval': 700, 'exploration_fraction': 0.2527628569570819, 'exploration_final_eps': 0.0344401750720791, 'net_arch': [128, 128, 128]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa538099817247f7990568453a666808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the server.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-28 11:11:30,543] Trial 2 finished with value: -1.9538459999999997 and parameters: {'learning_rate': 0.0002126835385636654, 'buffer_size': 20000, 'learning_starts': 1572, 'batch_size': 32, 'gamma': 0.983206386839582, 'train_freq': 4, 'target_update_interval': 700, 'exploration_fraction': 0.2527628569570819, 'exploration_final_eps': 0.0344401750720791, 'net_arch': [128, 128, 128]}. Best is trial 0 with value: -1.7576924999999999.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prueba 2 - Recompensa media: -1.95 ± 0.00\n",
      "WebSocket connection closed.\n",
      "Entorno cerrado para prueba 2\n",
      "\n",
      "=== Iniciando Prueba 3 ===\n",
      "Parámetros: {'learning_rate': 0.0002900584347262353, 'buffer_size': 20000, 'learning_starts': 1840, 'batch_size': 128, 'gamma': 0.9847659218699816, 'train_freq': 4, 'target_update_interval': 900, 'exploration_fraction': 0.1265120133579294, 'exploration_final_eps': 0.041049006361395565, 'net_arch': [256, 256, 256]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdfe0389340c453db1bc0ddb79037d1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the server.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-28 11:25:28,061] Trial 3 finished with value: -1.7830768 and parameters: {'learning_rate': 0.0002900584347262353, 'buffer_size': 20000, 'learning_starts': 1840, 'batch_size': 128, 'gamma': 0.9847659218699816, 'train_freq': 4, 'target_update_interval': 900, 'exploration_fraction': 0.1265120133579294, 'exploration_final_eps': 0.041049006361395565, 'net_arch': [256, 256, 256]}. Best is trial 0 with value: -1.7576924999999999.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prueba 3 - Recompensa media: -1.78 ± 0.10\n",
      "WebSocket connection closed.\n",
      "Entorno cerrado para prueba 3\n",
      "\n",
      "=== Iniciando Prueba 4 ===\n",
      "Parámetros: {'learning_rate': 0.00027425860905154616, 'buffer_size': 50000, 'learning_starts': 625, 'batch_size': 64, 'gamma': 0.9945812824181031, 'train_freq': 4, 'target_update_interval': 500, 'exploration_fraction': 0.1590347366647204, 'exploration_final_eps': 0.09831641578100747, 'net_arch': [128, 128, 128]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987fdaeba8554f35a18efe7c8b55312e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the server.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-28 11:38:34,582] Trial 4 finished with value: -1.6915382 and parameters: {'learning_rate': 0.00027425860905154616, 'buffer_size': 50000, 'learning_starts': 625, 'batch_size': 64, 'gamma': 0.9945812824181031, 'train_freq': 4, 'target_update_interval': 500, 'exploration_fraction': 0.1590347366647204, 'exploration_final_eps': 0.09831641578100747, 'net_arch': [128, 128, 128]}. Best is trial 4 with value: -1.6915382.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prueba 4 - Recompensa media: -1.69 ± 0.05\n",
      "WebSocket connection closed.\n",
      "Entorno cerrado para prueba 4\n",
      "\n",
      "=== Iniciando Prueba 5 ===\n",
      "Parámetros: {'learning_rate': 0.00044536718617693813, 'buffer_size': 10000, 'learning_starts': 1198, 'batch_size': 64, 'gamma': 0.9819027797057402, 'train_freq': 1, 'target_update_interval': 300, 'exploration_fraction': 0.17552302564914704, 'exploration_final_eps': 0.04894852099120832, 'net_arch': [256, 256]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f11a9bdb47b416681dfb1f323db3aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the server.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-28 11:58:36,821] Trial 5 finished with value: -1.6661536000000001 and parameters: {'learning_rate': 0.00044536718617693813, 'buffer_size': 10000, 'learning_starts': 1198, 'batch_size': 64, 'gamma': 0.9819027797057402, 'train_freq': 1, 'target_update_interval': 300, 'exploration_fraction': 0.17552302564914704, 'exploration_final_eps': 0.04894852099120832, 'net_arch': [256, 256]}. Best is trial 5 with value: -1.6661536000000001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prueba 5 - Recompensa media: -1.67 ± 0.12\n",
      "WebSocket connection closed.\n",
      "Entorno cerrado para prueba 5\n",
      "\n",
      "=== Iniciando Prueba 6 ===\n",
      "Parámetros: {'learning_rate': 0.00029525764668225435, 'buffer_size': 10000, 'learning_starts': 647, 'batch_size': 128, 'gamma': 0.9803990118930473, 'train_freq': 4, 'target_update_interval': 200, 'exploration_fraction': 0.17950997950630015, 'exploration_final_eps': 0.09161446176803974, 'net_arch': [128, 128]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff26876676b4452cbd5c5167f64c0209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the server.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-28 12:10:44,103] Trial 6 finished with value: -1.7584616 and parameters: {'learning_rate': 0.00029525764668225435, 'buffer_size': 10000, 'learning_starts': 647, 'batch_size': 128, 'gamma': 0.9803990118930473, 'train_freq': 4, 'target_update_interval': 200, 'exploration_fraction': 0.17950997950630015, 'exploration_final_eps': 0.09161446176803974, 'net_arch': [128, 128]}. Best is trial 5 with value: -1.6661536000000001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prueba 6 - Recompensa media: -1.76 ± 0.10\n",
      "WebSocket connection closed.\n",
      "Entorno cerrado para prueba 6\n",
      "\n",
      "=== Iniciando Prueba 7 ===\n",
      "Parámetros: {'learning_rate': 0.00011113732984144203, 'buffer_size': 20000, 'learning_starts': 1345, 'batch_size': 32, 'gamma': 0.9919418670231126, 'train_freq': 1, 'target_update_interval': 800, 'exploration_fraction': 0.2864075345230672, 'exploration_final_eps': 0.05281606207195465, 'net_arch': [128, 128, 128]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f464ff43d88461bbff3e4448971fcd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the server.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-28 12:28:39,893] Trial 7 finished with value: -0.27155120000000005 and parameters: {'learning_rate': 0.00011113732984144203, 'buffer_size': 20000, 'learning_starts': 1345, 'batch_size': 32, 'gamma': 0.9919418670231126, 'train_freq': 1, 'target_update_interval': 800, 'exploration_fraction': 0.2864075345230672, 'exploration_final_eps': 0.05281606207195465, 'net_arch': [128, 128, 128]}. Best is trial 7 with value: -0.27155120000000005.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prueba 7 - Recompensa media: -0.27 ± 1.30\n",
      "WebSocket connection closed.\n",
      "Entorno cerrado para prueba 7\n",
      "\n",
      "=== Iniciando Prueba 8 ===\n",
      "Parámetros: {'learning_rate': 0.00011625567383985442, 'buffer_size': 10000, 'learning_starts': 1629, 'batch_size': 32, 'gamma': 0.9826077927514182, 'train_freq': 4, 'target_update_interval': 400, 'exploration_fraction': 0.11015447470974996, 'exploration_final_eps': 0.08450454918382569, 'net_arch': [128, 128]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b067956aa4344acaa86d90977f111aad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the server.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-28 12:39:47,011] Trial 8 finished with value: -1.6615379999999997 and parameters: {'learning_rate': 0.00011625567383985442, 'buffer_size': 10000, 'learning_starts': 1629, 'batch_size': 32, 'gamma': 0.9826077927514182, 'train_freq': 4, 'target_update_interval': 400, 'exploration_fraction': 0.11015447470974996, 'exploration_final_eps': 0.08450454918382569, 'net_arch': [128, 128]}. Best is trial 7 with value: -0.27155120000000005.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prueba 8 - Recompensa media: -1.66 ± 0.00\n",
      "WebSocket connection closed.\n",
      "Entorno cerrado para prueba 8\n",
      "\n",
      "=== Iniciando Prueba 9 ===\n",
      "Parámetros: {'learning_rate': 0.00044373212683403266, 'buffer_size': 50000, 'learning_starts': 1889, 'batch_size': 32, 'gamma': 0.992670240055762, 'train_freq': 1, 'target_update_interval': 500, 'exploration_fraction': 0.26842569382031006, 'exploration_final_eps': 0.05133591830671913, 'net_arch': [256, 256]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcad541e8f7249039c1faa0ae668b0f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the server.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-28 12:59:20,225] Trial 9 finished with value: -1.6884615 and parameters: {'learning_rate': 0.00044373212683403266, 'buffer_size': 50000, 'learning_starts': 1889, 'batch_size': 32, 'gamma': 0.992670240055762, 'train_freq': 1, 'target_update_interval': 500, 'exploration_fraction': 0.26842569382031006, 'exploration_final_eps': 0.05133591830671913, 'net_arch': [256, 256]}. Best is trial 7 with value: -0.27155120000000005.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prueba 9 - Recompensa media: -1.69 ± 0.10\n",
      "WebSocket connection closed.\n",
      "Entorno cerrado para prueba 9\n",
      "\n",
      "=== Iniciando Prueba 10 ===\n",
      "Parámetros: {'learning_rate': 0.00010448061120979998, 'buffer_size': 20000, 'learning_starts': 1177, 'batch_size': 64, 'gamma': 0.9980347542563842, 'train_freq': 1, 'target_update_interval': 1000, 'exploration_fraction': 0.298399038540478, 'exploration_final_eps': 0.07067158133357831, 'net_arch': [128, 128, 128]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30428c44c8b84beab3cce476877b9acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the server.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-28 13:17:43,040] Trial 10 finished with value: -1.5584618 and parameters: {'learning_rate': 0.00010448061120979998, 'buffer_size': 20000, 'learning_starts': 1177, 'batch_size': 64, 'gamma': 0.9980347542563842, 'train_freq': 1, 'target_update_interval': 1000, 'exploration_fraction': 0.298399038540478, 'exploration_final_eps': 0.07067158133357831, 'net_arch': [128, 128, 128]}. Best is trial 7 with value: -0.27155120000000005.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prueba 10 - Recompensa media: -1.56 ± 0.11\n",
      "WebSocket connection closed.\n",
      "Entorno cerrado para prueba 10\n",
      "\n",
      "=== Iniciando Prueba 11 ===\n",
      "Parámetros: {'learning_rate': 0.00010332267955044316, 'buffer_size': 20000, 'learning_starts': 995, 'batch_size': 64, 'gamma': 0.9986216755038884, 'train_freq': 1, 'target_update_interval': 1000, 'exploration_fraction': 0.29561531533304036, 'exploration_final_eps': 0.06430917107987116, 'net_arch': [128, 128, 128]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e5dc4f5ced14fb1bf8bf37d69eea107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the server.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-28 13:36:31,093] Trial 11 finished with value: -1.6115381 and parameters: {'learning_rate': 0.00010332267955044316, 'buffer_size': 20000, 'learning_starts': 995, 'batch_size': 64, 'gamma': 0.9986216755038884, 'train_freq': 1, 'target_update_interval': 1000, 'exploration_fraction': 0.29561531533304036, 'exploration_final_eps': 0.06430917107987116, 'net_arch': [128, 128, 128]}. Best is trial 7 with value: -0.27155120000000005.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prueba 11 - Recompensa media: -1.61 ± 0.10\n",
      "WebSocket connection closed.\n",
      "Entorno cerrado para prueba 11\n",
      "\n",
      "=== Iniciando Prueba 12 ===\n",
      "Parámetros: {'learning_rate': 0.00014442425236346925, 'buffer_size': 20000, 'learning_starts': 985, 'batch_size': 64, 'gamma': 0.9981616742798969, 'train_freq': 1, 'target_update_interval': 1000, 'exploration_fraction': 0.2970656780851614, 'exploration_final_eps': 0.0766722739148692, 'net_arch': [128, 128, 128]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea73e0f474a452aafe4f708006030dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the server.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-28 13:55:07,619] Trial 12 finished with value: -1.6476922999999999 and parameters: {'learning_rate': 0.00014442425236346925, 'buffer_size': 20000, 'learning_starts': 985, 'batch_size': 64, 'gamma': 0.9981616742798969, 'train_freq': 1, 'target_update_interval': 1000, 'exploration_fraction': 0.2970656780851614, 'exploration_final_eps': 0.0766722739148692, 'net_arch': [128, 128, 128]}. Best is trial 7 with value: -0.27155120000000005.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prueba 12 - Recompensa media: -1.65 ± 0.02\n",
      "WebSocket connection closed.\n",
      "Entorno cerrado para prueba 12\n",
      "\n",
      "=== Iniciando Prueba 13 ===\n",
      "Parámetros: {'learning_rate': 0.00014535714135395687, 'buffer_size': 20000, 'learning_starts': 1250, 'batch_size': 64, 'gamma': 0.9896655641256908, 'train_freq': 1, 'target_update_interval': 800, 'exploration_fraction': 0.21158850207797938, 'exploration_final_eps': 0.021344527526953753, 'net_arch': [128, 128, 128]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a937d748147f47239d98c49b4d53b092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the server.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-28 14:13:05,004] Trial 13 finished with value: -1.7261541999999999 and parameters: {'learning_rate': 0.00014535714135395687, 'buffer_size': 20000, 'learning_starts': 1250, 'batch_size': 64, 'gamma': 0.9896655641256908, 'train_freq': 1, 'target_update_interval': 800, 'exploration_fraction': 0.21158850207797938, 'exploration_final_eps': 0.021344527526953753, 'net_arch': [128, 128, 128]}. Best is trial 7 with value: -0.27155120000000005.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prueba 13 - Recompensa media: -1.73 ± 0.07\n",
      "WebSocket connection closed.\n",
      "Entorno cerrado para prueba 13\n",
      "\n",
      "=== Iniciando Prueba 14 ===\n",
      "Parámetros: {'learning_rate': 0.00013616096340685716, 'buffer_size': 20000, 'learning_starts': 927, 'batch_size': 128, 'gamma': 0.9949759137517417, 'train_freq': 1, 'target_update_interval': 700, 'exploration_fraction': 0.27658933467168667, 'exploration_final_eps': 0.06172002409980354, 'net_arch': [256, 256, 256]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e355fd4657a84ba794512e44d2e7e50b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the server.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-28 14:42:14,164] Trial 14 finished with value: -1.5923079999999998 and parameters: {'learning_rate': 0.00013616096340685716, 'buffer_size': 20000, 'learning_starts': 927, 'batch_size': 128, 'gamma': 0.9949759137517417, 'train_freq': 1, 'target_update_interval': 700, 'exploration_fraction': 0.27658933467168667, 'exploration_final_eps': 0.06172002409980354, 'net_arch': [256, 256, 256]}. Best is trial 7 with value: -0.27155120000000005.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prueba 14 - Recompensa media: -1.59 ± 0.00\n",
      "WebSocket connection closed.\n",
      "Entorno cerrado para prueba 14\n",
      "\n",
      "=== Optimización Completada ===\n",
      "Mejor prueba: 7\n",
      "Mejor recompensa: -0.2716\n",
      "Mejores hiperparámetros: {'learning_rate': 0.00011113732984144203, 'buffer_size': 20000, 'learning_starts': 1345, 'batch_size': 32, 'gamma': 0.9919418670231126, 'train_freq': 1, 'target_update_interval': 800, 'exploration_fraction': 0.2864075345230672, 'exploration_final_eps': 0.05281606207195465, 'net_arch': [128, 128, 128]}\n",
      "Resultados guardados en ./optuna_studies/dqn_pvpoke_optuna_20250428_103714\n",
      "\n",
      "=== Entrenamiento Final con Mejores Hiperparámetros ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1411422b041949d98d74d776540808a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the server.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo final guardado en ./optuna_studies/dqn_pvpoke_optuna_20250428_103714/dqn_pvpoke_final_Mantine-Gligar_optuna\n",
      "WebSocket connection closed.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import numpy as np\n",
    "import os\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import json\n",
    "from ClassPVPOKE import PVPokeEnv\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "# Asegurar funcionamiento de asyncio anidado\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Configuración del estudio\n",
    "STUDY_NAME = \"dqn_pvpoke_optuna\"\n",
    "N_TRIALS = 15  # Número de combinaciones de hiperparámetros a probar\n",
    "TRAINING_TIMESTEPS = 100000  # Pasos de entrenamiento por prueba (reducido para búsqueda)\n",
    "EVAL_EPISODES = 10  # Episodios de evaluación por prueba\n",
    "POKEMON_MATCHUP = \"Mantine-Gligar\"\n",
    "BATTLE_FORMAT = \"1v1\"\n",
    "\n",
    "# Crear directorios para resultados\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "base_dir = f\"./optuna_studies/{STUDY_NAME}_{timestamp}\"\n",
    "log_dir = f\"{base_dir}/logs\"\n",
    "models_dir = f\"{base_dir}/models\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Definir la función objetivo para optimización\n",
    "def objective(trial):\n",
    "    # Identificador único para esta prueba\n",
    "    trial_id = f\"trial_{trial.number}\"\n",
    "    trial_log_dir = f\"{log_dir}/{trial_id}\"\n",
    "    trial_model_dir = f\"{models_dir}/{trial_id}\"\n",
    "    os.makedirs(trial_log_dir, exist_ok=True)\n",
    "    os.makedirs(trial_model_dir, exist_ok=True)\n",
    "    \n",
    "    # Espacio de búsqueda para hiperparámetros (ajustado para episodios de ~35 pasos)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 5e-4, log=True)\n",
    "    buffer_size = trial.suggest_categorical('buffer_size', [10000, 20000, 50000])\n",
    "    learning_starts = trial.suggest_int('learning_starts', 500, 2000)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    gamma = trial.suggest_float('gamma', 0.98, 0.999)\n",
    "    train_freq = trial.suggest_categorical('train_freq', [1, 4])\n",
    "    target_update_interval = trial.suggest_int('target_update_interval', 200, 1000, step=100)\n",
    "    exploration_fraction = trial.suggest_float('exploration_fraction', 0.1, 0.3)\n",
    "    exploration_final_eps = trial.suggest_float('exploration_final_eps', 0.02, 0.1)\n",
    "    \n",
    "    # Arquitectura de la red\n",
    "    net_arch_options = [\n",
    "        [128, 128],\n",
    "        [256, 256],\n",
    "        [128, 128, 128],\n",
    "        [256, 256, 256]\n",
    "    ]\n",
    "    net_arch = trial.suggest_categorical('net_arch', net_arch_options)\n",
    "    \n",
    "    print(f\"\\n=== Iniciando Prueba {trial.number} ===\")\n",
    "    print(f\"Parámetros: {trial.params}\")\n",
    "    \n",
    "    try:\n",
    "        # Crear y conectar al entorno\n",
    "        env = PVPokeEnv(\"ws://localhost:8000/ws\", \"notebook\", \"pvpoke\", battle_format=BATTLE_FORMAT)\n",
    "        env.loop.run_until_complete(env.connect())\n",
    "        env = Monitor(env)\n",
    "        \n",
    "        # Crear modelo DQN con los hiperparámetros de esta prueba\n",
    "        model = DQN(\n",
    "            policy=\"MlpPolicy\",\n",
    "            env=env,\n",
    "            learning_rate=learning_rate,\n",
    "            buffer_size=buffer_size,\n",
    "            learning_starts=learning_starts,\n",
    "            batch_size=batch_size,\n",
    "            gamma=gamma,\n",
    "            train_freq=train_freq,\n",
    "            gradient_steps=1,\n",
    "            target_update_interval=target_update_interval,\n",
    "            exploration_fraction=exploration_fraction,\n",
    "            exploration_initial_eps=1.0,\n",
    "            exploration_final_eps=exploration_final_eps,\n",
    "            tensorboard_log=trial_log_dir,\n",
    "            policy_kwargs={\"net_arch\": net_arch},\n",
    "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        \n",
    "        # Callback para evaluación periódica\n",
    "        eval_callback = EvalCallback(\n",
    "            env,\n",
    "            best_model_save_path=trial_model_dir,\n",
    "            log_path=trial_log_dir,\n",
    "            eval_freq=5000,\n",
    "            deterministic=True,\n",
    "            render=False,\n",
    "            n_eval_episodes=5,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Entrenar el modelo\n",
    "        model.learn(\n",
    "            total_timesteps=TRAINING_TIMESTEPS,\n",
    "            callback=eval_callback,\n",
    "            progress_bar=True\n",
    "        )\n",
    "        \n",
    "        # Evaluación final\n",
    "        mean_reward, std_reward = evaluate_policy(\n",
    "            model, \n",
    "            env, \n",
    "            n_eval_episodes=EVAL_EPISODES,\n",
    "            deterministic=True\n",
    "        )\n",
    "        \n",
    "        # Guardar modelo final\n",
    "        model.save(f\"{trial_model_dir}/final_model\")\n",
    "        \n",
    "        # Reportar métricas\n",
    "        print(f\"Prueba {trial.number} - Recompensa media: {mean_reward:.2f} ± {std_reward:.2f}\")\n",
    "        \n",
    "        # Registrar métricas adicionales\n",
    "        with open(f\"{trial_model_dir}/results.json\", \"w\") as f:\n",
    "            json.dump({\n",
    "                \"trial_number\": trial.number,\n",
    "                \"params\": {k: str(v) if isinstance(v, list) else v for k, v in trial.params.items()},\n",
    "                \"mean_reward\": float(mean_reward),\n",
    "                \"std_reward\": float(std_reward)\n",
    "            }, f)\n",
    "        \n",
    "        return mean_reward\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error en prueba {trial.number}: {e}\")\n",
    "        return float('-inf')  # Peor valor posible en caso de error\n",
    "        \n",
    "    finally:\n",
    "        # Siempre cerrar el entorno para liberar recursos\n",
    "        try:\n",
    "            env.close()\n",
    "            print(f\"Entorno cerrado para prueba {trial.number}\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Crear estudio para optimización\n",
    "study = optuna.create_study(\n",
    "    study_name=STUDY_NAME,\n",
    "    direction=\"maximize\",  # Maximizar recompensa\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=5)  # Detener pruebas poco prometedoras\n",
    ")\n",
    "\n",
    "# Iniciar proceso de optimización\n",
    "print(f\"Iniciando optimización con {N_TRIALS} pruebas\")\n",
    "study.optimize(objective, n_trials=N_TRIALS)\n",
    "\n",
    "# Imprimir y guardar resultados\n",
    "print(\"\\n=== Optimización Completada ===\")\n",
    "print(f\"Mejor prueba: {study.best_trial.number}\")\n",
    "print(f\"Mejor recompensa: {study.best_value:.4f}\")\n",
    "print(f\"Mejores hiperparámetros: {study.best_params}\")\n",
    "\n",
    "# Guardar mejores hiperparámetros\n",
    "with open(f\"{base_dir}/best_params.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"best_trial\": study.best_trial.number,\n",
    "        \"best_reward\": float(study.best_value),\n",
    "        \"best_params\": {k: str(v) if isinstance(v, list) else v for k, v in study.best_params.items()}\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"Resultados guardados en {base_dir}\")\n",
    "\n",
    "# Código para crear modelo final con mejores hiperparámetros\n",
    "print(\"\\n=== Entrenamiento Final con Mejores Hiperparámetros ===\")\n",
    "try:\n",
    "    # Crear entorno para entrenamiento final\n",
    "    env = PVPokeEnv(\"ws://localhost:8000/ws\", \"notebook\", \"pvpoke\", battle_format=BATTLE_FORMAT)\n",
    "    env.loop.run_until_complete(env.connect())\n",
    "    env = Monitor(env)\n",
    "    \n",
    "    # Copiar mejores parámetros (para manipularlos)\n",
    "    best_params = study.best_params.copy()\n",
    "    \n",
    "    # Extraer arquitectura de red (que es una lista)\n",
    "    net_arch = best_params.pop('net_arch')\n",
    "    \n",
    "    # Crear modelo con mejores hiperparámetros\n",
    "    best_model = DQN(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=env,\n",
    "        **best_params,  # Desempaquetar mejores hiperparámetros\n",
    "        policy_kwargs={\"net_arch\": net_arch},\n",
    "        tensorboard_log=f\"{base_dir}/final_model_logs\",\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "    \n",
    "    # Entrenar modelo final por más tiempo\n",
    "    best_model.learn(\n",
    "        total_timesteps=1000000,  # Entrenar por más tiempo\n",
    "        progress_bar=True\n",
    "    )\n",
    "    \n",
    "    # Guardar modelo final\n",
    "    final_model_path = f\"{base_dir}/dqn_pvpoke_final_{POKEMON_MATCHUP}_optuna\"\n",
    "    best_model.save(final_model_path)\n",
    "    print(f\"Modelo final guardado en {final_model_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error en entrenamiento final: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Cerrar entorno\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting DQN training: DQN/Mantine-Gligar_128-128-128_100k\n",
      "Logs will be saved to: ./logs/1v1/1/1m/DQN/Mantine-Gligar_128-128-128_100k\n",
      "Models will be saved to: ./models/1v1/1/1m/dqn/Mantine-Gligar/128-128-128_100k_v2\n",
      "Connected to the server.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8311dd5ae932407498b82d9198ed8214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Final model saved to ./models/1v1/1/1m/dqn/Mantine-Gligar/128-128-128_100k_v2/dqn_pvpoke_final\n",
      "Closing environment...\n",
      "WebSocket connection closed.\n",
      "Training session ended.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "from ClassPVPOKE import PVPokeEnv\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import torch\n",
    "\n",
    "# Apply nest_asyncio to avoid asyncio event loop issues\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Create experiment directory with timestamp\n",
    "# Define descriptive experiment naming\n",
    "pokemon_matchup = \"Mantine-Gligar\"\n",
    "\n",
    "battle_format = \"1v1\"\n",
    "model_type = \"DQN\"  \n",
    "network_arch = \"128-128-128\"\n",
    "version = \"v2\"\n",
    "buffer = \"100k\"\n",
    "step = \"1m\"\n",
    "train_frec = \"1\"\n",
    "\n",
    "# Build more meaningful directory structure\n",
    "experiment_name = f\"{model_type}/{pokemon_matchup}_{network_arch}_{buffer}\"\n",
    "log_dir = f\"./logs/{battle_format}/{train_frec}/{step}/{experiment_name}\"\n",
    "models_dir = f\"./models/{battle_format}/{train_frec}/{step}/{model_type.lower()}/{pokemon_matchup}/{network_arch}_{buffer}_{version}\"\n",
    "\n",
    "print(f\"Starting DQN training: {experiment_name}\")\n",
    "print(f\"Logs will be saved to: {log_dir}\")\n",
    "print(f\"Models will be saved to: {models_dir}\")\n",
    "print(f\"CUda available: {torch.cuda.is_available()}\")\n",
    "try:\n",
    "    # Create and connect to environment\n",
    "    env = PVPokeEnv(\"ws://localhost:8000/ws\", \"notebook\", \"pvpoke\", battle_format=\"1v1\")\n",
    "    env.loop.run_until_complete(env.connect())\n",
    "    env = Monitor(env)\n",
    "    \n",
    "    # Set up the DQN agent\n",
    "    model = DQN(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=env,\n",
    "        learning_rate=1e-3,             # Slightly lower to ensure stable learning\n",
    "        buffer_size=100000,             # Sufficient for your state space, smaller than 250k\n",
    "        learning_starts=2000,           # Allow collecting more diverse experiences before learning\n",
    "        batch_size=128,                 # Good balance for your use case\n",
    "        gamma=0.999,                     # Standard discount factor for short episodes\n",
    "        train_freq=4,                   # Update every step for more frequent learning\n",
    "        gradient_steps=1,\n",
    "        target_update_interval=500,     # More frequent target updates\n",
    "        exploration_fraction=0.2,       # Explore more at the beginning\n",
    "        exploration_initial_eps=1.0,\n",
    "        exploration_final_eps=0.05,     # Slightly higher to maintain some exploration\n",
    "        tensorboard_log=log_dir,\n",
    "        policy_kwargs={\"net_arch\": [128, 128,128]},  # Larger network for the complex game logic\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    )\n",
    "    \n",
    "    # Save checkpoints during training\n",
    "    checkpoint_callback = CheckpointCallback(\n",
    "        save_freq=5000,                    # Save every 5000 steps\n",
    "        save_path=models_dir,\n",
    "        name_prefix='dqn_pvpoke'\n",
    "    )\n",
    "    \n",
    "    # Train the agent\n",
    "    print(\"Starting training...\")\n",
    "    model.learn(\n",
    "        total_timesteps=2000000,\n",
    "        callback=checkpoint_callback,\n",
    "        progress_bar=True\n",
    "    )\n",
    "    \n",
    "    # Save the final model\n",
    "    final_model_path = f\"{models_dir}/dqn_pvpoke_final\"\n",
    "    model.save(final_model_path)\n",
    "    print(f\"Training completed. Final model saved to {final_model_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during training: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Always close the environment\n",
    "    print(\"Closing environment...\")\n",
    "    env.close()\n",
    "    print(\"Training session ended.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the server.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "696db5f75dcc41c5b16933eecb4e2bd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from ClassPVPOKE import PVPokeEnv\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "\n",
    "env = PVPokeEnv(\"ws://localhost:8000/ws\", \"notebook\", \"pvpoke\")\n",
    "env.loop.run_until_complete(env.connect())\n",
    "\n",
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env,\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=128,\n",
    "    batch_size=64,\n",
    "    n_epochs=10,\n",
    "    gamma=0.999,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.2,\n",
    "    ent_coef=0.01,\n",
    "    vf_coef=0.5,\n",
    "    max_grad_norm=0.5,\n",
    "    tensorboard_log=\"./experiments/Mantine-Gligar-Terminal-1/DQN/1v1/5lvl/128\",\n",
    "    policy_kwargs={\"net_arch\": [128, 128]}  \n",
    ")\n",
    "\n",
    "# Guardar checkpoints regularmente\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=5000, \n",
    "    save_path='./models/ppo/mandibuzz-Annihilape', \n",
    "    name_prefix='ppo_pvpoke_1v1'\n",
    ")\n",
    "\n",
    "# Entrenar con los parámetros ajustados\n",
    "model.learn(total_timesteps=200000, callback=checkpoint_callback, progress_bar=True)\n",
    "\n",
    "# Guardar el modelo final\n",
    "model.save(\"ppo_pvpoke_final_mandibuzz-Annihilape\")\n",
    "\n",
    "# Close the environment connection\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the server.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "517cde8ce93948819d5f1bc376b0c1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import asyncio\n",
    "import websockets\n",
    "from pydantic import BaseModel\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import json\n",
    "from ClassPVPOKE import PVPokeEnv\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Crear y conectar el entorno\n",
    "env = PVPokeEnv(\"ws://localhost:8000/ws\", \"notebook\", \"pvpoke\")\n",
    "env.loop.run_until_complete(env.connect())\n",
    "\n",
    "\n",
    "\n",
    "# ✅ Crear el modelo DQN con la red personalizada\n",
    "model = DQN(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env,\n",
    "    learning_rate=1e-3,              # Aprendizaje ajustado para 1v1\n",
    "    buffer_size=50000,               # Tamaño de buffer aumentado para estabilidad\n",
    "    learning_starts=1000,            # Más pasos antes de comenzar el aprendizaje\n",
    "    batch_size=128,                  # Tamaño de batch ideal para 1v1\n",
    "    gamma=0.99,                      # Factor de descuento a largo plazo\n",
    "    train_freq=4,                    # Actualización cada 4 pasos\n",
    "    gradient_steps=1,                # Un paso de gradiente por actualización\n",
    "    target_update_interval=1000,     # Actualización del target más frecuente\n",
    "    exploration_fraction=0.1,        \n",
    "    exploration_initial_eps=1.0,     \n",
    "    exploration_final_eps=0.05,      \n",
    "    tensorboard_log=\"./dqn_pvpoke_tensorboard/mandibuzz-Annihilape\",\n",
    "    policy_kwargs={\"net_arch\": [128, 128]}  # ✅ Especificando la red de 128-128\n",
    ")\n",
    "\n",
    "# ✅ Callback para guardar checkpoints cada 5000 steps\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=5000, \n",
    "    save_path='./models/dqn/mandibuzz-Annihilape', \n",
    "    name_prefix='dqn_pvpoke'\n",
    ")\n",
    "\n",
    "# ✅ Entrenar el modelo con los nuevos ajustes\n",
    "model.learn(total_timesteps=200000, callback=checkpoint_callback, progress_bar=True)\n",
    "\n",
    "# ✅ Guardar el modelo final\n",
    "model.save(\"dqn_pvpoke_final_mandibuzz-Annihilape\")\n",
    "\n",
    "# Cerrar el entorno al finalizar\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenar DQN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the server.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[WinError 123] El nombre de archivo, el nombre de directorio o la sintaxis de la etiqueta del volumen no son correctos: './models/dqn/Skar-TA-SA-BRB-Swampert-MS-HC-SW-Umbreon-SN-LR-FP-Trev-SC-SeB-SD-Vigp-Co-BS-RS-Dewgong-Is-IW-DR\"'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 41\u001b[0m\n\u001b[0;32m     36\u001b[0m checkpoint_callback \u001b[38;5;241m=\u001b[39m CheckpointCallback(save_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m, \n\u001b[0;32m     37\u001b[0m                                          save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./models/dqn/Skar-TA-SA-BRB-Swampert-MS-HC-SW-Umbreon-SN-LR-FP-Trev-SC-SeB-SD-Vigp-Co-BS-RS-Dewgong-Is-IW-DR\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     38\u001b[0m                                          name_prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdqn_pvpoke\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Train the agent\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Save final model\u001b[39;00m\n\u001b[0;32m     44\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdqn_pvpoke_final_Skar-TA-SA-BRB-Swampert-MS-HC-SW-Umbreon-SN-LR-FP-Trev-SC-SeB-SD-Vigp-Co-BS-RS-Dewgong-Is-IW-DR\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\samue\\miniconda3\\envs\\gym\\Lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:267\u001b[0m, in \u001b[0;36mDQN.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfDQN,\n\u001b[0;32m    260\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    265\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    266\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfDQN:\n\u001b[1;32m--> 267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\samue\\miniconda3\\envs\\gym\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:314\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfOffPolicyAlgorithm,\n\u001b[0;32m    307\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    312\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    313\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfOffPolicyAlgorithm:\n\u001b[1;32m--> 314\u001b[0m     total_timesteps, callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_learn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    322\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must set the environment before calling learn()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\samue\\miniconda3\\envs\\gym\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:297\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm._setup_learn\u001b[1;34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_noise \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mnum_envs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_noise, VectorizedActionNoise)\n\u001b[0;32m    294\u001b[0m ):\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_noise \u001b[38;5;241m=\u001b[39m VectorizedActionNoise(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_noise, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mnum_envs)\n\u001b[1;32m--> 297\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_learn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\samue\\miniconda3\\envs\\gym\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:434\u001b[0m, in \u001b[0;36mBaseAlgorithm._setup_learn\u001b[1;34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mconfigure_logger(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensorboard_log, tb_log_name, reset_num_timesteps)\n\u001b[0;32m    433\u001b[0m \u001b[38;5;66;03m# Create eval callback if needed\u001b[39;00m\n\u001b[1;32m--> 434\u001b[0m callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_timesteps, callback\n",
      "File \u001b[1;32mc:\\Users\\samue\\miniconda3\\envs\\gym\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:380\u001b[0m, in \u001b[0;36mBaseAlgorithm._init_callback\u001b[1;34m(self, callback, progress_bar)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m progress_bar:\n\u001b[0;32m    378\u001b[0m     callback \u001b[38;5;241m=\u001b[39m CallbackList([callback, ProgressBarCallback()])\n\u001b[1;32m--> 380\u001b[0m \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m callback\n",
      "File \u001b[1;32mc:\\Users\\samue\\miniconda3\\envs\\gym\\Lib\\site-packages\\stable_baselines3\\common\\callbacks.py:73\u001b[0m, in \u001b[0;36mBaseCallback.init_callback\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;124;03mInitialize the callback by saving references to the\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03mRL model and the training environment for convenience.\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m---> 73\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\samue\\miniconda3\\envs\\gym\\Lib\\site-packages\\stable_baselines3\\common\\callbacks.py:286\u001b[0m, in \u001b[0;36mCheckpointCallback._init_callback\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_init_callback\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;66;03m# Create folder if needed\u001b[39;00m\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 286\u001b[0m         \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen os>:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 123] El nombre de archivo, el nombre de directorio o la sintaxis de la etiqueta del volumen no son correctos: './models/dqn/Skar-TA-SA-BRB-Swampert-MS-HC-SW-Umbreon-SN-LR-FP-Trev-SC-SeB-SD-Vigp-Co-BS-RS-Dewgong-Is-IW-DR\"'"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import asyncio\n",
    "import websockets\n",
    "from pydantic import BaseModel\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import json\n",
    "from ClassPVPOKE import PVPokeEnv\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create and connect environment\n",
    "env = PVPokeEnv(\"ws://localhost:8000/ws\", \"notebook\", \"pvpoke\")\n",
    "env.loop.run_until_complete(env.connect())\n",
    "\n",
    "# Create DQN agent with appropriate hyperparameters\n",
    "model = DQN(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    learning_rate=1e-3,  # Aumentar la tasa de aprendizaje\n",
    "    buffer_size=50000,  # Reducir el tamaño del buffer\n",
    "    learning_starts=100,  # Reducir el número de pasos antes de empezar a aprender\n",
    "    batch_size=128,  # Reducir el tamaño del batch\n",
    "    gamma=0.99,\n",
    "    train_freq=4,\n",
    "    gradient_steps=1,\n",
    "    target_update_interval=1000,  # Reducir el intervalo de actualización del objetivo\n",
    "    exploration_fraction=0.1,\n",
    "    exploration_initial_eps=1,\n",
    "    exploration_final_eps=0.05,    \n",
    "    tensorboard_log=\"./dqn_pvpoke_tensorboard/3v3/Skar-TA-SA-BRB-Swampert-MS-HC-SW-Umbreon-SN-LR-FP-Trev-SC-SeB-SD-Vigp-Co-BS-RS-Dewgong-Is-IW-DR\"\n",
    ")\n",
    "# Create checkpoint callback\n",
    "checkpoint_callback = CheckpointCallback(save_freq=5000, \n",
    "                                         save_path='./models/dqn/3v3/Skar-TA-SA-BRB-Swampert-MS-HC-SW-Umbreon-SN-LR-FP-Trev-SC-SeB-SD-Vigp-Co-BS-RS-Dewgong-Is-IW-DR\"',\n",
    "                                         name_prefix='dqn_pvpoke')\n",
    "\n",
    "# Train the agent\n",
    "model.learn(total_timesteps = 200000, callback=checkpoint_callback, progress_bar=False)\n",
    "\n",
    "# Save final model\n",
    "model.save(\"dqn_pvpoke_final_Skar-TA-SA-BRB-Swampert-MS-HC-SW-Umbreon-SN-LR-FP-Trev-SC-SeB-SD-Vigp-Co-BS-RS-Dewgong-Is-IW-DR\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probar el agente en un episodio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the server.\n",
      "Loaded model from C:\\pvpokeDRL\\pvpoke\\PVPOKE\\models\\1v1\\1\\500k\\dqn\\Mantine-Gligar\\128-128-128_100k_v2\\dqn_pvpoke_final.zip\n",
      "\n",
      "Episode 1 started\n",
      "Initial observation: [0 5 1 0 5 1]\n",
      "\n",
      "Action taken: 0\n",
      "Observation: [0 5 1 0 5 1]\n",
      "Reward: -0.0015132408575031508\n",
      "\n",
      "Action taken: 0\n",
      "Observation: [1 5 1 1 5 1]\n",
      "Reward: -0.0015132408575031508\n",
      "\n",
      "Action taken: 0\n",
      "Observation: [1 5 1 1 5 1]\n",
      "Reward: -0.0015132408575031508\n",
      "\n",
      "Action taken: 0\n",
      "Observation: [2 5 1 2 5 1]\n",
      "Reward: -0.0015132408575031508\n",
      "\n",
      "Action taken: 0\n",
      "Observation: [2 4 1 2 4 1]\n",
      "Reward: -0.0015132408575031508\n",
      "\n",
      "Action taken: 0\n",
      "Observation: [2 3 1 0 4 1]\n",
      "Reward: -0.23770491803278687\n",
      "\n",
      "Action taken: 0\n",
      "Observation: [2 3 1 0 4 1]\n",
      "Reward: 0.023076923076923078\n",
      "\n",
      "Action taken: 0\n",
      "Observation: [3 3 1 0 4 1]\n",
      "Reward: -0.0015132408575031508\n",
      "\n",
      "Action taken: 0\n",
      "Observation: [3 3 1 1 4 1]\n",
      "Reward: -0.0015132408575031508\n",
      "\n",
      "Action taken: 0\n",
      "Observation: [4 3 1 1 4 1]\n",
      "Reward: -0.0015132408575031508\n",
      "\n",
      "Action taken: 0\n",
      "Observation: [4 3 1 2 4 1]\n",
      "Reward: -0.0015132408575031508\n",
      "\n",
      "Action taken: 0\n",
      "Observation: [4 3 1 2 4 1]\n",
      "Reward: -0.0015132408575031508\n",
      "\n",
      "Action taken: 0\n",
      "Observation: [4 1 1 0 4 1]\n",
      "Reward: -0.23770491803278687\n",
      "\n",
      "Action taken: 1\n",
      "Observation: [5 1 1 0 4 1]\n",
      "Reward: 0.023076923076923078\n",
      "\n",
      "Action taken: 1\n",
      "Observation: [3 1 1 0 4 0]\n",
      "Reward: 0.007692307692307693\n",
      "\n",
      "Action taken: 0\n",
      "Observation: [3 1 1 0 4 0]\n",
      "Reward: -0.02459016393442623\n",
      "\n",
      "Action taken: 0\n",
      "Observation: [3 1 1 1 3 0]\n",
      "Reward: -0.0015132408575031508\n",
      "\n",
      "Action taken: 1\n",
      "Observation: [1 1 1 1 3 0]\n",
      "Reward: 0.007692307692307693\n",
      "\n",
      "Action taken: 0\n",
      "Observation: [1 1 1 1 3 0]\n",
      "Reward: -0.02459016393442623\n",
      "\n",
      "Action taken: 0\n",
      "Observation: [2 1 1 2 3 0]\n",
      "Reward: -0.0015132408575031508\n",
      "\n",
      "Action taken: 3\n",
      "Observation: [2 1 1 2 3 0]\n",
      "Reward: 0\n",
      "\n",
      "Action taken: 3\n",
      "Observation: [2 1 1 2 3 0]\n",
      "Reward: -0.02459016393442623\n",
      "\n",
      "Action taken: 3\n",
      "Observation: [2 1 0 0 3 0]\n",
      "Reward: -0.00819672131147541\n",
      "\n",
      "Action taken: 0\n",
      "Observation: [2 1 0 0 3 0]\n",
      "Reward: -0.0015132408575031508\n",
      "\n",
      "Action taken: 0\n",
      "Observation: [2 0 0 1 3 0]\n",
      "Reward: -0.0015132408575031508\n",
      "\n",
      "Action taken: 0\n",
      "Observation: [3 0 0 1 3 0]\n",
      "Reward: -0.0015132408575031508\n",
      "\n",
      "Action taken: 2\n",
      "Observation: [0 0 0 1 0 0]\n",
      "Reward: 0.5923076923076923\n",
      "\n",
      "Action taken: 1\n",
      "Observation: [0 0 0 1 0 0]\n",
      "Reward: 0\n",
      "\n",
      "Action taken: 1\n",
      "Observation: [0 0 0 1 0 0]\n",
      "Reward: 1\n",
      "\n",
      "Episode finished with total reward: 1.0737704918032787\n",
      "Actions taken in episode: [array(0, dtype=int64), array(0, dtype=int64), array(0, dtype=int64), array(0, dtype=int64), array(0, dtype=int64), array(0, dtype=int64), array(0, dtype=int64), array(0, dtype=int64), array(0, dtype=int64), array(0, dtype=int64), array(0, dtype=int64), array(0, dtype=int64), array(0, dtype=int64), array(1, dtype=int64), array(1, dtype=int64), array(0, dtype=int64), array(0, dtype=int64), array(1, dtype=int64), array(0, dtype=int64), array(0, dtype=int64), array(3, dtype=int64), array(3, dtype=int64), array(3, dtype=int64), array(0, dtype=int64), array(0, dtype=int64), array(0, dtype=int64), array(2, dtype=int64), array(1, dtype=int64), array(1, dtype=int64)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WebSocket connection closed.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import websockets\n",
    "import json\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "import numpy as np\n",
    "from ClassPVPOKE import PVPokeEnv\n",
    "from stable_baselines3 import DQN\n",
    "import matplotlib.pyplot as plt\n",
    "import nest_asyncio \n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "# Create and connect environment\n",
    "env = PVPokeEnv(\"ws://localhost:8000/ws\", \"notebook\", \"pvpoke\",battle_format=\"1v1\")\n",
    "env.loop.run_until_complete(env.connect())\n",
    "\n",
    "# Load pre-trained model\n",
    "model_path = \"C:\\pvpokeDRL\\pvpoke\\PVPOKE\\models\\\\1v1\\\\1\\\\500k\\dqn\\Mantine-Gligar\\\\128-128-128_100k_v2\\dqn_pvpoke_final.zip\"\n",
    "model = DQN.load(model_path)\n",
    "\n",
    "#model_path  = \"C:\\pvpokeDRL\\pvpoke\\PVPOKE\\models\\ppo\\mandibuzz-Annihilape\\ppo_pvpoke_1v1_140000_steps.zip\"\n",
    "#model = PPO.load(model_path)\n",
    "print(f\"Loaded model from {model_path}\")\n",
    "\n",
    "# Test model performance with debugging\n",
    "rewards = []\n",
    "actions_taken = []\n",
    "observations = []\n",
    "\n",
    "for episode in range(1):\n",
    "    obs, info = env.reset()\n",
    "    print(f\"\\nEpisode {episode + 1} started\")\n",
    "    print(f\"Initial observation: {obs}\")\n",
    "    \n",
    "    \n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    episode_actions = []\n",
    "    \n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        episode_actions.append(action)\n",
    "        print(f\"\\nAction taken: {action}\")\n",
    "        \n",
    "        obs, reward, done, terminated, info = env.step(action)\n",
    "        print(f\"Observation: {obs}\")\n",
    "        print(f\"Reward: {reward}\")\n",
    "        \n",
    "        total_reward += reward\n",
    "        observations.append(obs)\n",
    "    \n",
    "    print(f\"\\nEpisode finished with total reward: {total_reward}\")\n",
    "    print(f\"Actions taken in episode: {episode_actions}\")\n",
    "    rewards.append(total_reward)\n",
    "    actions_taken.extend(episode_actions)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(rewards, marker='o')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('DQN Agent Performance')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(actions_taken, bins=np.arange(env.action_space.n + 1) - 0.5, align=\"mid\", rwidth=0.8)\n",
    "plt.xticks(np.arange(env.action_space.n))\n",
    "plt.xlabel('Actions')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Action Distribution')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Close environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the server.\n",
      "Environment Specifications:\n",
      "Observation Space: Box(0.0, 255.0, (8,), float32)\n",
      "Action Space: Discrete(4)\n",
      "\n",
      "Model Architecture:\n",
      "DQNPolicy(\n",
      "  (q_net): QNetwork(\n",
      "    (features_extractor): FlattenExtractor(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    )\n",
      "    (q_net): Sequential(\n",
      "      (0): Linear(in_features=8, out_features=64, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=64, out_features=4, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (q_net_target): QNetwork(\n",
      "    (features_extractor): FlattenExtractor(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    )\n",
      "    (q_net): Sequential(\n",
      "      (0): Linear(in_features=8, out_features=64, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=64, out_features=4, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Sending message: reset\n",
      "\n",
      "Test Prediction:\n",
      "Observation shape: (8,)\n",
      "Predicted action: 0\n",
      "WebSocket connection closed.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import numpy as np\n",
    "from stable_baselines3 import DQN\n",
    "from ClassPVPOKE import PVPokeEnv\n",
    "\n",
    "# Create environment\n",
    "env = PVPokeEnv(\"ws://localhost:8000/ws\", \"notebook\", \"pvpoke\")\n",
    "env.loop.run_until_complete(env.connect())\n",
    "\n",
    "# Print environment specs\n",
    "print(\"Environment Specifications:\")\n",
    "print(f\"Observation Space: {env.observation_space}\")\n",
    "print(f\"Action Space: {env.action_space}\")\n",
    "\n",
    "# Load model and print architecture\n",
    "model_path = \"dqn_pvpoke_final_mantine-gligar_optuna\"\n",
    "try:\n",
    "    model = DQN.load(model_path, env=env)\n",
    "    print(\"\\nModel Architecture:\")\n",
    "    print(model.policy)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "\n",
    "# Test single prediction\n",
    "try:\n",
    "    obs, _ = env.reset()\n",
    "    print(\"\\nTest Prediction:\")\n",
    "    print(f\"Observation shape: {obs.shape}\")\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    print(f\"Predicted action: {action}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during prediction: {e}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTUNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-28 11:51:52,284] A new study created in memory with name: no-name-cf637b24-c188-4629-bac1-95c458b508dd\n",
      "C:\\Users\\samue\\AppData\\Local\\Temp\\ipykernel_31864\\890825868.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)\n",
      "C:\\Users\\samue\\AppData\\Local\\Temp\\ipykernel_31864\\890825868.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  gamma = trial.suggest_uniform('gamma', 0.9, 0.999)\n",
      "C:\\Users\\samue\\AppData\\Local\\Temp\\ipykernel_31864\\890825868.py:20: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  exploration_fraction = trial.suggest_uniform('exploration_fraction', 0.1, 0.5)\n",
      "C:\\Users\\samue\\AppData\\Local\\Temp\\ipykernel_31864\\890825868.py:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  exploration_final_eps = trial.suggest_uniform('exploration_final_eps', 0.01, 0.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the server.\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samue\\miniconda3\\envs\\gym\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=500, episode_reward=-133.00 +/- 0.00\n",
      "Episode length: 46.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=1000, episode_reward=-73.60 +/- 7.68\n",
      "Episode length: 35.80 +/- 1.47\n",
      "New best mean reward!\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=1500, episode_reward=-76.20 +/- 7.60\n",
      "Episode length: 30.00 +/- 2.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=2000, episode_reward=-83.00 +/- 0.00\n",
      "Episode length: 34.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=2500, episode_reward=-88.00 +/- 0.00\n",
      "Episode length: 31.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=3000, episode_reward=-98.60 +/- 18.80\n",
      "Episode length: 37.00 +/- 2.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=3500, episode_reward=-48.00 +/- 0.00\n",
      "Episode length: 31.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=4000, episode_reward=-67.00 +/- 0.00\n",
      "Episode length: 28.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=4500, episode_reward=-94.00 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=5000, episode_reward=-76.00 +/- 0.00\n",
      "Episode length: 27.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=5500, episode_reward=-42.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=6000, episode_reward=-133.00 +/- 0.00\n",
      "Episode length: 46.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=6500, episode_reward=-133.00 +/- 0.00\n",
      "Episode length: 45.80 +/- 0.40\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=7000, episode_reward=-133.00 +/- 0.00\n",
      "Episode length: 46.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=7500, episode_reward=-115.20 +/- 10.78\n",
      "Episode length: 39.60 +/- 2.94\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=8000, episode_reward=-133.00 +/- 0.00\n",
      "Episode length: 46.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=8500, episode_reward=-127.00 +/- 0.00\n",
      "Episode length: 31.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=9000, episode_reward=-133.00 +/- 0.00\n",
      "Episode length: 46.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=9500, episode_reward=-111.00 +/- 0.00\n",
      "Episode length: 39.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=10000, episode_reward=-102.00 +/- 0.00\n",
      "Episode length: 36.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-28 13:35:13,096] Trial 0 finished with value: -102.0 and parameters: {'learning_rate': 0.0007484760104390964, 'buffer_size': 34306, 'batch_size': 256, 'gamma': 0.9536373641947591, 'exploration_fraction': 0.3608392572521345, 'exploration_final_eps': 0.08839072268939926}. Best is trial 0 with value: -102.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WebSocket connection closed.\n",
      "Connected to the server.\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=500, episode_reward=-133.00 +/- 0.00\n",
      "Episode length: 46.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=1000, episode_reward=-115.60 +/- 1.20\n",
      "Episode length: 40.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=1500, episode_reward=-106.00 +/- 0.00\n",
      "Episode length: 39.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=2000, episode_reward=-109.00 +/- 0.00\n",
      "Episode length: 38.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=2500, episode_reward=-86.60 +/- 4.80\n",
      "Episode length: 22.80 +/- 3.60\n",
      "New best mean reward!\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=3000, episode_reward=-77.00 +/- 0.00\n",
      "Episode length: 32.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=3500, episode_reward=-93.00 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=4000, episode_reward=-83.00 +/- 0.00\n",
      "Episode length: 32.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=4500, episode_reward=-124.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=5000, episode_reward=-93.00 +/- 19.60\n",
      "Episode length: 38.20 +/- 1.47\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=5500, episode_reward=-133.00 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=6000, episode_reward=-51.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=6500, episode_reward=-82.00 +/- 14.00\n",
      "Episode length: 25.20 +/- 4.40\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=7000, episode_reward=-89.00 +/- 0.00\n",
      "Episode length: 32.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=7500, episode_reward=-96.00 +/- 0.00\n",
      "Episode length: 34.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=8000, episode_reward=-99.00 +/- 0.00\n",
      "Episode length: 35.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=8500, episode_reward=-133.00 +/- 0.00\n",
      "Episode length: 45.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=9000, episode_reward=-133.00 +/- 0.00\n",
      "Episode length: 46.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=9500, episode_reward=-77.80 +/- 12.40\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=10000, episode_reward=-85.80 +/- 12.40\n",
      "Episode length: 35.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-28 15:09:40,186] Trial 1 finished with value: -88.9 and parameters: {'learning_rate': 9.301996110892225e-05, 'buffer_size': 80607, 'batch_size': 32, 'gamma': 0.9713104497888039, 'exploration_fraction': 0.45181342894106047, 'exploration_final_eps': 0.07901715989387353}. Best is trial 1 with value: -88.9.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WebSocket connection closed.\n",
      "Connected to the server.\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=500, episode_reward=-82.00 +/- 0.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=1000, episode_reward=-93.00 +/- 0.00\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=1500, episode_reward=-78.80 +/- 10.78\n",
      "Episode length: 21.80 +/- 1.47\n",
      "New best mean reward!\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=2000, episode_reward=-43.60 +/- 76.25\n",
      "Episode length: 31.00 +/- 6.00\n",
      "New best mean reward!\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=2500, episode_reward=-48.00 +/- 0.00\n",
      "Episode length: 31.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=3000, episode_reward=-88.20 +/- 13.72\n",
      "Episode length: 28.60 +/- 0.49\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=3500, episode_reward=-64.20 +/- 12.40\n",
      "Episode length: 33.20 +/- 0.40\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=4000, episode_reward=-86.00 +/- 0.00\n",
      "Episode length: 32.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=4500, episode_reward=-50.20 +/- 59.60\n",
      "Episode length: 29.00 +/- 4.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=5000, episode_reward=-76.20 +/- 12.40\n",
      "Episode length: 36.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=5500, episode_reward=-75.20 +/- 13.72\n",
      "Episode length: 34.40 +/- 0.49\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=6000, episode_reward=-133.00 +/- 0.00\n",
      "Episode length: 46.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=6500, episode_reward=-121.00 +/- 0.00\n",
      "Episode length: 42.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=7000, episode_reward=-99.00 +/- 0.00\n",
      "Episode length: 35.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=7500, episode_reward=-88.80 +/- 12.40\n",
      "Episode length: 35.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=8000, episode_reward=-99.00 +/- 0.00\n",
      "Episode length: 35.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=8500, episode_reward=-127.00 +/- 0.00\n",
      "Episode length: 44.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=9000, episode_reward=-130.00 +/- 0.00\n",
      "Episode length: 45.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=9500, episode_reward=-124.00 +/- 0.00\n",
      "Episode length: 43.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=10000, episode_reward=-133.00 +/- 0.00\n",
      "Episode length: 46.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-28 16:38:21,748] Trial 2 finished with value: -133.0 and parameters: {'learning_rate': 0.0005002020345389054, 'buffer_size': 47961, 'batch_size': 256, 'gamma': 0.9748084210129654, 'exploration_fraction': 0.281023740501287, 'exploration_final_eps': 0.05193648699760344}. Best is trial 1 with value: -88.9.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WebSocket connection closed.\n",
      "Connected to the server.\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=500, episode_reward=-133.00 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=1000, episode_reward=-109.00 +/- 0.00\n",
      "Episode length: 41.80 +/- 0.40\n",
      "New best mean reward!\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=1500, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 37.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=2000, episode_reward=-84.40 +/- 4.80\n",
      "Episode length: 33.60 +/- 1.20\n",
      "New best mean reward!\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=2500, episode_reward=-96.00 +/- 0.00\n",
      "Episode length: 23.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=3000, episode_reward=-96.00 +/- 0.00\n",
      "Episode length: 23.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=3500, episode_reward=-83.00 +/- 0.00\n",
      "Episode length: 32.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=4000, episode_reward=-91.40 +/- 2.94\n",
      "Episode length: 33.40 +/- 0.49\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=4500, episode_reward=-80.00 +/- 0.00\n",
      "Episode length: 31.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=5000, episode_reward=-96.00 +/- 0.00\n",
      "Episode length: 34.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=5500, episode_reward=-67.60 +/- 18.42\n",
      "Episode length: 26.00 +/- 4.15\n",
      "New best mean reward!\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=6000, episode_reward=-55.40 +/- 2.80\n",
      "Episode length: 31.40 +/- 0.80\n",
      "New best mean reward!\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=6500, episode_reward=-52.00 +/- 56.00\n",
      "Episode length: 28.20 +/- 1.60\n",
      "New best mean reward!\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=7000, episode_reward=-115.00 +/- 0.00\n",
      "Episode length: 27.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=7500, episode_reward=-121.00 +/- 0.00\n",
      "Episode length: 42.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=8000, episode_reward=-87.20 +/- 11.60\n",
      "Episode length: 33.20 +/- 0.40\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=8500, episode_reward=-90.00 +/- 0.00\n",
      "Episode length: 32.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=9000, episode_reward=-93.00 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=9500, episode_reward=-56.60 +/- 5.20\n",
      "Episode length: 31.80 +/- 1.60\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=10000, episode_reward=-106.00 +/- 0.00\n",
      "Episode length: 37.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-28 18:20:11,287] Trial 3 finished with value: -106.0 and parameters: {'learning_rate': 0.00010056170468402817, 'buffer_size': 55247, 'batch_size': 32, 'gamma': 0.9955249082006237, 'exploration_fraction': 0.20645174081023848, 'exploration_final_eps': 0.08701723354861225}. Best is trial 1 with value: -88.9.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WebSocket connection closed.\n",
      "Connected to the server.\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=500, episode_reward=-133.00 +/- 0.00\n",
      "Episode length: 46.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=1000, episode_reward=-124.00 +/- 0.00\n",
      "Episode length: 43.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=1500, episode_reward=-61.60 +/- 22.76\n",
      "Episode length: 33.00 +/- 1.90\n",
      "New best mean reward!\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=2000, episode_reward=-71.00 +/- 19.60\n",
      "Episode length: 23.20 +/- 1.47\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=2500, episode_reward=-79.00 +/- 19.60\n",
      "Episode length: 22.20 +/- 0.98\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=3000, episode_reward=-73.60 +/- 15.19\n",
      "Episode length: 21.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=3500, episode_reward=-78.40 +/- 13.29\n",
      "Episode length: 32.80 +/- 4.40\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=4000, episode_reward=-73.60 +/- 15.19\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=4500, episode_reward=-54.80 +/- 0.40\n",
      "Episode length: 20.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=5000, episode_reward=-67.20 +/- 20.00\n",
      "Episode length: 27.40 +/- 5.82\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=5500, episode_reward=-55.60 +/- 2.73\n",
      "Episode length: 20.80 +/- 0.40\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=6000, episode_reward=-56.80 +/- 3.43\n",
      "Episode length: 28.20 +/- 5.88\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=6500, episode_reward=-54.40 +/- 0.49\n",
      "Episode length: 20.60 +/- 0.49\n",
      "New best mean reward!\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=7000, episode_reward=-44.40 +/- 4.80\n",
      "Episode length: 29.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=7500, episode_reward=-54.00 +/- 0.00\n",
      "Episode length: 31.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=8000, episode_reward=-51.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=8500, episode_reward=-51.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=9000, episode_reward=-51.00 +/- 0.00\n",
      "Episode length: 31.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=9500, episode_reward=-48.60 +/- 1.20\n",
      "Episode length: 29.20 +/- 0.40\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Eval num_timesteps=10000, episode_reward=-48.00 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n",
      "Sending message: reset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-28 20:03:57,849] Trial 4 finished with value: -48.0 and parameters: {'learning_rate': 3.2243775007763094e-05, 'buffer_size': 23335, 'batch_size': 32, 'gamma': 0.9425402178303517, 'exploration_fraction': 0.27687288025671425, 'exploration_final_eps': 0.012090955143464824}. Best is trial 4 with value: -48.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WebSocket connection closed.\n",
      "Best hyperparameters:  {'learning_rate': 3.2243775007763094e-05, 'buffer_size': 23335, 'batch_size': 32, 'gamma': 0.9425402178303517, 'exploration_fraction': 0.27687288025671425, 'exploration_final_eps': 0.012090955143464824}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import asyncio\n",
    "import websockets\n",
    "from pydantic import BaseModel\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import json\n",
    "from ClassPVPOKE import PVPokeEnv\n",
    "\n",
    "# Define the objective function\n",
    "def objective(trial):\n",
    "    # Define the hyperparameter search space\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)\n",
    "    buffer_size = trial.suggest_int('buffer_size', 10000, 100000)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    gamma = trial.suggest_uniform('gamma', 0.9, 0.999)\n",
    "    exploration_fraction = trial.suggest_uniform('exploration_fraction', 0.1, 0.5)\n",
    "    exploration_final_eps = trial.suggest_uniform('exploration_final_eps', 0.01, 0.1)\n",
    "    \n",
    "    # Create and connect environment\n",
    "    env = PVPokeEnv(\"ws://localhost:8000/ws\", \"notebook\", \"pvpoke\")\n",
    "    env.loop.run_until_complete(env.connect())\n",
    "    \n",
    "    # Create the DQN model\n",
    "    model = DQN(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        learning_rate=learning_rate,\n",
    "        buffer_size=buffer_size,\n",
    "        learning_starts=500,\n",
    "        batch_size=batch_size,\n",
    "        gamma=gamma,\n",
    "        train_freq=4,\n",
    "        gradient_steps=1,\n",
    "        target_update_interval=500,\n",
    "        exploration_fraction=exploration_fraction,\n",
    "        exploration_initial_eps=1.0,\n",
    "        exploration_final_eps=exploration_final_eps,\n",
    "        tensorboard_log=\"./dqn_pvpoke_tensorboard/\"\n",
    "    )\n",
    "    \n",
    "    # Create evaluation callback\n",
    "    eval_callback = EvalCallback(env, best_model_save_path='./logs/',\n",
    "                                 log_path='./logs/', eval_freq=500,\n",
    "                                 deterministic=True, render=False)\n",
    "    \n",
    "    # Train the model\n",
    "    model.learn(total_timesteps=10000, callback=eval_callback)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "    \n",
    "    # Close the environment\n",
    "    env.close()\n",
    "    \n",
    "    return mean_reward\n",
    "\n",
    "# Create the study and optimize the objective function\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=5)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters: \", study.best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
